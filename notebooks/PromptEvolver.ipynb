{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afec9ee5",
   "metadata": {},
   "source": [
    "# Prompt Evolver - Interactive Notebook\n",
    "\n",
    "**Automated prompt optimization and testing for LLM workflows**\n",
    "\n",
    "This notebook guides you through optimizing prompts using the Prompt Evolver pipeline. You'll:\n",
    "1. Configure your environment and LLM backend\n",
    "2. Run the optimization pipeline on example data\n",
    "3. Analyze results and improvements\n",
    "\n",
    "**Prerequisites:** Python 3.10+, virtual environment activated\n",
    "\n",
    "**Time to complete:** ~5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc230be",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "---\n",
    "\n",
    "**Option A: Automated Setup (Recommended)**\n",
    "```bash\n",
    "./setup.sh  # Unix/macOS/Linux\n",
    "setup.bat   # Windows\n",
    "```\n",
    "\n",
    "**Option B: Manual Setup**\n",
    "```bash\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate  # Unix/macOS\n",
    "# .venv\\Scripts\\activate    # Windows\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Important:** After setup, select the `.venv` Python interpreter as your notebook kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe777cff",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "---\n",
    "\n",
    "Configure paths, model names, and pipeline parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a0f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/bru1t/Documents/Development/projects/prompt-evolver\n",
      "\n",
      "Configuration loaded:\n",
      "  Data source: Example\n",
      "  Max generations: 3\n",
      "  Models: mistralai/ministral-3-3b\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# === 2.1 Auto-detect project root ===\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"Locate project root by searching for src/prompt_evolver.\"\"\"\n",
    "    for parent in [start, *start.parents]:\n",
    "        if (parent / \"src\" / \"prompt_evolver\").exists():\n",
    "            return parent\n",
    "    raise RuntimeError(\"Could not locate project root containing src/prompt_evolver\")\n",
    "\n",
    "project_root = find_project_root(Path.cwd())\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# === 2.2 Data paths ===\n",
    "DATA_DIR = project_root / \"data\"\n",
    "CONFIG_DIR = project_root / \"configs\"\n",
    "\n",
    "# Example datasets (pre-configured for testing)\n",
    "EXAMPLE_PROMPTS = DATA_DIR / \"example.prompts.csv\"\n",
    "EXAMPLE_TEXTS = DATA_DIR / \"example.texts.csv\"\n",
    "EXAMPLE_TASKS = DATA_DIR / \"example.tasks.csv\"\n",
    "EXAMPLE_RESULTS = DATA_DIR / \"example.results.csv\"\n",
    "\n",
    "# Your custom datasets (optional)\n",
    "CUSTOM_PROMPTS = DATA_DIR / \"prompts.csv\"\n",
    "CUSTOM_TEXTS = DATA_DIR / \"texts.csv\"\n",
    "CUSTOM_TASKS = DATA_DIR / \"tasks.csv\"\n",
    "CUSTOM_RESULTS = DATA_DIR / \"results.csv\"\n",
    "\n",
    "# Prompt templates\n",
    "EXECUTION_TEMPLATE = CONFIG_DIR / \"prompts\" / \"prompt.execute.md\"\n",
    "IMPROVEMENT_TEMPLATE = CONFIG_DIR / \"prompts\" / \"prompt.improve.md\"\n",
    "EVALUATION_TEMPLATE = CONFIG_DIR / \"prompts\" / \"prompt.evaluation.md\"\n",
    "\n",
    "# Configuration file\n",
    "CONFIG_FILE = CONFIG_DIR / \"config.yaml\"\n",
    "\n",
    "# === 2.3 Model configuration ===\n",
    "# Update these based on your LLM backend:\n",
    "# - Ollama/LM Studio: \"mistralai/ministral-3-3b\", \"llama3.2\", etc.\n",
    "# - OpenAI API: \"gpt-4\", \"gpt-3.5-turbo\", etc.\n",
    "\n",
    "EXECUTION_MODEL = \"mistralai/ministral-3-3b\"\n",
    "IMPROVEMENT_MODEL = \"mistralai/ministral-3-3b\"\n",
    "EVALUATION_MODEL = \"mistralai/ministral-3-3b\"\n",
    "\n",
    "# === 2.4 Pipeline parameters ===\n",
    "MAX_GENERATIONS = 3  # Maximum improvement iterations per task\n",
    "USE_EXAMPLE_DATA = True  # Toggle between example and custom data\n",
    "\n",
    "print(f\"\\nConfiguration loaded:\")\n",
    "print(f\"  Data source: {'Example' if USE_EXAMPLE_DATA else 'Custom'}\")\n",
    "print(f\"  Max generations: {MAX_GENERATIONS}\")\n",
    "print(f\"  Models: {EXECUTION_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb30a7",
   "metadata": {},
   "source": [
    "## 3. Run Pipeline\n",
    "---\n",
    "\n",
    "Execute the optimization pipeline. This will:\n",
    "- Load prompts, texts, and tasks from CSV files\n",
    "- Run each task and evaluate outputs\n",
    "- Iteratively improve prompts based on feedback\n",
    "- Generate a results CSV with before/after comparison\n",
    "\n",
    "**Expected runtime:** ~2-3 minutes (depends on LLM speed)\n",
    "\n",
    "**Note:** If you modified code in `src/`, restart the kernel before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c159ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 15:53:15,578 [INFO] Start pipeline prompts=/Users/bru1t/Documents/Development/projects/prompt-evolver/data/example.prompts.csv texts=/Users/bru1t/Documents/Development/projects/prompt-evolver/data/example.texts.csv tasks=/Users/bru1t/Documents/Development/projects/prompt-evolver/data/example.tasks.csv output=/Users/bru1t/Documents/Development/projects/prompt-evolver/data/example.results.csv\n",
      "2026-01-23 15:53:15,617 [INFO] Processing 16 tasks...\n",
      "2026-01-23 15:53:15,617 [INFO] Task 1/16 (6%) - Processing task_id=task_001\n",
      "2026-01-23 15:53:15,618 [INFO] Start task id_task=task_001 id_prompt=prompt_001 id_text=text_001 type=Writing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline with example.prompts.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 15:53:22,246 [WARNING] Eval failed task=task_001 score=0.00 issues=['invalid_json']\n",
      "2026-01-23 15:53:22,249 [INFO] Iteration start task=task_001 iter=1\n",
      "2026-01-23 15:53:25,439 [INFO] Accept prompt task=task_001 iter=1 score=0.95 tokens_delta=6\n",
      "2026-01-23 15:53:25,439 [INFO] End task id_task=task_001 status=pass iterations_used=1\n",
      "2026-01-23 15:53:25,439 [INFO] Task 1/16 (6%) - Completed: iterations=1, token_delta=+6, leakage=NO\n",
      "2026-01-23 15:53:25,440 [INFO] Task 2/16 (12%) - Processing task_id=task_002\n",
      "2026-01-23 15:53:25,440 [INFO] Start task id_task=task_002 id_prompt=prompt_002 id_text=text_002 type=Extraction\n",
      "2026-01-23 15:53:27,138 [WARNING] Eval failed task=task_002 score=0.00 issues=[\"Missing required key 'status' at top level (should be a flat value instead of nested under `status`).\", 'Nested structure under `status` is not compliant with expected schema.', 'evaluator_leakage_filtered']\n",
      "2026-01-23 15:53:27,138 [INFO] Iteration start task=task_002 iter=1\n",
      "2026-01-23 15:53:28,390 [INFO] Accept prompt task=task_002 iter=1 score=1.00 tokens_delta=27\n",
      "2026-01-23 15:53:28,391 [INFO] End task id_task=task_002 status=pass iterations_used=1\n",
      "2026-01-23 15:53:28,392 [INFO] Task 2/16 (12%) - Completed: iterations=1, token_delta=+27, leakage=NO\n",
      "2026-01-23 15:53:28,392 [INFO] Task 3/16 (18%) - Processing task_id=task_003\n",
      "2026-01-23 15:53:28,393 [INFO] Start task id_task=task_003 id_prompt=prompt_003 id_text=text_003 type=Editing\n",
      "2026-01-23 15:53:31,300 [WARNING] Eval failed task=task_003 score=0.65 issues=['The output does not fully match a single concise paragraph format (it is more of an explanatory critique).', 'Lacks the direct, neutral statement focused on speed and reliability as required.', 'Does not eliminate unnecessary repetition or simplify technical details into a clear, streamlined sentence.', 'The phrasing remains descriptive rather than presenting a polished marketing draft.']\n",
      "2026-01-23 15:53:31,301 [INFO] Iteration start task=task_003 iter=1\n",
      "2026-01-23 15:53:34,905 [WARNING] Reject prompt task=task_003 iter=1 score=0.65 issues=['Exceeds the required single concise paragraph format (too verbose).', \"Repeats core claims about speed ('processing requests in milliseconds' and 'eliminating redundancy').\", \"Includes unnecessary marketing language ('unmatched', 'seamless, efficient interactions') that dilutes technical clarity.\", \"Mixes general benefits with specific technical details ('advanced Mistral AI technology'), which feels off-topic for a simple description of speed/reliability.\"]\n",
      "2026-01-23 15:53:34,906 [INFO] Iteration start task=task_003 iter=2\n",
      "2026-01-23 15:53:38,235 [WARNING] Reject prompt task=task_003 iter=2 score=0.65 issues=[\"The output still includes marketing language ('delivering instant responses', 'seamless real-time performance') which deviates from a neutral, concise focus on speed and reliability.\", \"Redundancy persists (e.g., 'eliminates redundancy' is mentioned twice in the same sentence).\", \"Technical phrasing ('maintaining technical precision') feels overly verbose for a simple claim about speed.\", \"The structure does not prioritize the main benefit ('speed and reliability') in the first line as required.\"]\n",
      "2026-01-23 15:53:38,236 [INFO] End task id_task=task_003 status=no_improvement iterations_used=2\n",
      "2026-01-23 15:53:38,236 [INFO] Task 3/16 (18%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:53:38,237 [INFO] Task 4/16 (25%) - Processing task_id=task_004\n",
      "2026-01-23 15:53:38,237 [INFO] Start task id_task=task_004 id_prompt=prompt_004 id_text=text_004 type=Comparison\n",
      "2026-01-23 15:53:41,097 [WARNING] Eval failed task=task_004 score=0.60 issues=[\"Missing 'Price' comparison (expected format: '1) Price: Product A vs Product B').\", \"Minor formatting inconsistency in warranty duration ('Product A has a **2-year warranty**' vs expected concise phrasing).\", \"Use of bullet points instead of numbered list for 'Usage Rating' and 'Carrying Case Quality'.\"]\n",
      "2026-01-23 15:53:41,098 [INFO] Iteration start task=task_004 iter=1\n",
      "2026-01-23 15:53:41,885 [WARNING] Sanity check failed task=task_004 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:53:41,886 [INFO] Iteration start task=task_004 iter=2\n",
      "2026-01-23 15:53:45,521 [WARNING] Reject prompt task=task_004 iter=2 score=0.75 issues=[\"Missing ';' after warranty length descriptions (e.g., 'two-year warranty' should be 'two-year warranty;').\", \"The phrasing 'Product A is cheaper at **$10**' is slightly less concise than the expected output ('Product A costs $10').\", \"No explicit mention of 'while' in the first item (expected: 'while Product B costs') for smoother flow.\"]\n",
      "2026-01-23 15:53:45,522 [INFO] End task id_task=task_004 status=no_improvement iterations_used=2\n",
      "2026-01-23 15:53:45,523 [INFO] Task 4/16 (25%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:53:45,523 [INFO] Task 5/16 (31%) - Processing task_id=task_005\n",
      "2026-01-23 15:53:45,524 [INFO] Start task id_task=task_005 id_prompt=prompt_005 id_text=text_005 type=Evaluation\n",
      "2026-01-23 15:53:46,141 [INFO] Eval passed task=task_005 score=1.00\n",
      "2026-01-23 15:53:46,142 [INFO] Iteration start task=task_005 iter=1\n",
      "2026-01-23 15:53:47,043 [WARNING] Reject prompt task=task_005 iter=1 score=1.00 issues=[]\n",
      "2026-01-23 15:53:47,043 [INFO] End task id_task=task_005 status=pass iterations_used=1\n",
      "2026-01-23 15:53:47,044 [INFO] Task 5/16 (31%) - Completed: iterations=1, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:53:47,045 [INFO] Task 6/16 (37%) - Processing task_id=task_006\n",
      "2026-01-23 15:53:47,046 [INFO] Start task id_task=task_006 id_prompt=prompt_006 id_text=text_006 type=Extraction\n",
      "2026-01-23 15:53:48,755 [WARNING] Eval failed task=task_006 score=0.60 issues=[\"Included extra whitespace after 'users' value ('45k active users')\", \"Missing comma after 'revenue' key\"]\n",
      "2026-01-23 15:53:48,756 [INFO] Iteration start task=task_006 iter=1\n",
      "2026-01-23 15:53:49,470 [WARNING] Sanity check failed task=task_006 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:53:49,470 [INFO] Iteration start task=task_006 iter=2\n",
      "2026-01-23 15:53:51,034 [INFO] Accept prompt task=task_006 iter=2 score=1.00 tokens_delta=37\n",
      "2026-01-23 15:53:51,034 [INFO] End task id_task=task_006 status=pass iterations_used=2\n",
      "2026-01-23 15:53:51,035 [INFO] Task 6/16 (37%) - Completed: iterations=2, token_delta=+37, leakage=NO\n",
      "2026-01-23 15:53:51,035 [INFO] Task 7/16 (43%) - Processing task_id=task_007\n",
      "2026-01-23 15:53:51,036 [INFO] Start task id_task=task_007 id_prompt=prompt_007 id_text=text_007 type=Research\n",
      "2026-01-23 15:53:54,568 [WARNING] Eval failed task=task_007 score=0.50 issues=['The **takeaways** do not match the required format (missing explicit mention of side effects and limitations). The original output omitted critical safety considerations.', 'The open question does not fully align with the expected phrasing—it lacks emphasis on *long-term persistence* of efficacy and *acceptability of side effects* in diverse populations.', 'The task’s short-term focus is not sufficiently expanded to address long-term implications (e.g., cumulative risks, metabolic changes) despite the open question’s scope.']\n",
      "2026-01-23 15:53:54,571 [INFO] Iteration start task=task_007 iter=1\n",
      "2026-01-23 15:53:55,680 [WARNING] Sanity check failed task=task_007 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:53:55,681 [INFO] Iteration start task=task_007 iter=2\n",
      "2026-01-23 15:53:56,563 [WARNING] Sanity check failed task=task_007 iter=2 reason=too_large_increase\n",
      "2026-01-23 15:53:56,564 [INFO] End task id_task=task_007 status=sanity_check:too_large_increase iterations_used=2\n",
      "2026-01-23 15:53:56,565 [INFO] Task 7/16 (43%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:53:56,565 [INFO] Task 8/16 (50%) - Processing task_id=task_008\n",
      "2026-01-23 15:53:56,566 [INFO] Start task id_task=task_008 id_prompt=prompt_008 id_text=text_008 type=Ops\n",
      "2026-01-23 15:54:03,109 [WARNING] Eval failed task=task_008 score=0.75 issues=['Step 1 is redundant with Step 3 (low-traffic scheduling is implied by canary rollout timing).', 'Missing explicit mention of **pre-deployment validation** (e.g., code review, dependency checks) before feature flag activation.', 'Steps 4–6 lack clarity on **monitoring thresholds** for error rates/latency (e.g., ‘<0.5%’ should be explicitly defined).', 'Step 7’s rollback plan is vague—should specify **revert mechanism** (e.g., feature flag toggle, database rollback) and **notification criteria**.', 'Step 8’s documentation focus is broad; could prioritize **automated logging** of critical metrics (e.g., A/B test results).']\n",
      "2026-01-23 15:54:03,112 [INFO] Iteration start task=task_008 iter=1\n",
      "2026-01-23 15:54:04,685 [WARNING] Sanity check failed task=task_008 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:54:04,685 [INFO] Iteration start task=task_008 iter=2\n",
      "2026-01-23 15:54:06,022 [WARNING] Sanity check failed task=task_008 iter=2 reason=too_large_increase\n",
      "2026-01-23 15:54:06,023 [INFO] End task id_task=task_008 status=sanity_check:too_large_increase iterations_used=2\n",
      "2026-01-23 15:54:06,023 [INFO] Task 8/16 (50%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:06,024 [INFO] Task 9/16 (56%) - Processing task_id=task_009\n",
      "2026-01-23 15:54:06,024 [INFO] Start task id_task=task_009 id_prompt=prompt_009 id_text=text_009 type=Planning\n",
      "2026-01-23 15:54:09,097 [WARNING] Eval failed task=task_009 score=0.60 issues=[\"Missing required field: `owner` (should be 'Marta' and 'Ken')\", \"Missing third action from expected output ('Confirm legal terms with counsel')\", 'Due date for the third action is missing in provided output', \"Priority for the third action should be 'low' but not included\"]\n",
      "2026-01-23 15:54:09,099 [INFO] Iteration start task=task_009 iter=1\n",
      "2026-01-23 15:54:09,777 [WARNING] Sanity check failed task=task_009 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:54:09,778 [INFO] Iteration start task=task_009 iter=2\n",
      "2026-01-23 15:54:10,464 [WARNING] Sanity check failed task=task_009 iter=2 reason=too_large_increase\n",
      "2026-01-23 15:54:10,464 [INFO] End task id_task=task_009 status=sanity_check:too_large_increase iterations_used=2\n",
      "2026-01-23 15:54:10,465 [INFO] Task 9/16 (56%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:10,465 [INFO] Task 10/16 (62%) - Processing task_id=task_010\n",
      "2026-01-23 15:54:10,465 [INFO] Start task id_task=task_010 id_prompt=prompt_010 id_text=text_010 type=Redaction\n",
      "2026-01-23 15:54:13,530 [WARNING] Eval failed task=task_010 score=0.50 issues=['Missing introduction of the name ([REDACTED_NAME])', 'Missing account number reference ([REDACTED_ACCOUNT])', 'Incomplete redaction of contact details (only partial email/phone shown in output)', \"Greeting 'Hi Support' lacks a more formal or personalized opening like '[REDACTED_NAME]'\", \"The closing is missing ('Thanks!')\"]\n",
      "2026-01-23 15:54:13,531 [INFO] Iteration start task=task_010 iter=1\n",
      "2026-01-23 15:54:14,500 [WARNING] Sanity check failed task=task_010 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:54:14,501 [INFO] Iteration start task=task_010 iter=2\n",
      "2026-01-23 15:54:15,315 [WARNING] Sanity check failed task=task_010 iter=2 reason=too_large_increase\n",
      "2026-01-23 15:54:15,316 [INFO] End task id_task=task_010 status=sanity_check:too_large_increase iterations_used=2\n",
      "2026-01-23 15:54:15,317 [INFO] Task 10/16 (62%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:15,318 [INFO] Task 11/16 (68%) - Processing task_id=task_011\n",
      "2026-01-23 15:54:15,319 [INFO] Start task id_task=task_011 id_prompt=prompt_011 id_text=text_011 type=Consistency\n",
      "2026-01-23 15:54:17,577 [WARNING] Eval failed task=task_011 score=0.30 issues=['Sentence labels (`sentence1`, `sentence2`) do not match the required format (`sentence_a`, `sentence_b`).', 'The output does not include an overall consistency label (e.g., `overall` field).']\n",
      "2026-01-23 15:54:17,579 [INFO] Iteration start task=task_011 iter=1\n",
      "2026-01-23 15:54:18,372 [WARNING] Sanity check failed task=task_011 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:54:18,372 [INFO] Iteration start task=task_011 iter=2\n",
      "2026-01-23 15:54:21,770 [WARNING] Reject prompt task=task_011 iter=2 score=0.50 issues=[\"Missing 'contradictions' key in the root JSON structure (should be nested under 'overall').\", 'The provided `sentence_a` and `sentence_b` are correctly matched, but the output does not strictly adhere to the exact schema format where contradictions must be directly under `overall`.']\n",
      "2026-01-23 15:54:21,770 [INFO] End task id_task=task_011 status=no_improvement iterations_used=2\n",
      "2026-01-23 15:54:21,771 [INFO] Task 11/16 (68%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:21,771 [INFO] Task 12/16 (75%) - Processing task_id=task_012\n",
      "2026-01-23 15:54:21,772 [INFO] Start task id_task=task_012 id_prompt=prompt_012 id_text=text_012 type=SQL\n",
      "2026-01-23 15:54:24,649 [WARNING] Eval failed task=task_012 score=0.65 issues=['JOIN condition direction is incorrect (should be `o.customer_id = c.id` instead of `c.id = o.customer_id`).', \"Missing `c.id` in the `GROUP BY` clause (though functionally equivalent, it's not strictly required).\", 'Unnecessary `LEFT JOIN` when `MAX(o.order_date)` will work with a regular `JOIN` for customers with no orders.']\n",
      "2026-01-23 15:54:24,650 [INFO] Iteration start task=task_012 iter=1\n",
      "2026-01-23 15:54:27,462 [WARNING] Reject prompt task=task_012 iter=1 score=0.50 issues=['Missing `GROUP BY` clause (required for aggregation functions like `MAX`)', 'Incorrect join type: Uses implicit JOIN instead of explicit LEFT JOIN to ensure all customers are included even if they have no orders', 'Does not match expected format exactly (e.g., column ordering could be ambiguous, though functionally equivalent in this case)']\n",
      "2026-01-23 15:54:27,463 [INFO] Iteration start task=task_012 iter=2\n",
      "2026-01-23 15:54:30,598 [WARNING] Reject prompt task=task_012 iter=2 score=0.50 issues=['JOIN condition direction is reversed (`c.id = o.customer_id` vs `o.customer_id = c.id`). Both are logically equivalent but the expected output uses the latter style (more conventional for readability).', 'Missing `c.id` in `GROUP BY` clause (though functionally correct due to LEFT JOIN, it should be included for consistency with SQL best practices).']\n",
      "2026-01-23 15:54:30,599 [INFO] End task id_task=task_012 status=no_improvement iterations_used=2\n",
      "2026-01-23 15:54:30,599 [INFO] Task 12/16 (75%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:30,600 [INFO] Task 13/16 (81%) - Processing task_id=task_013\n",
      "2026-01-23 15:54:30,600 [INFO] Start task id_task=task_013 id_prompt=prompt_013 id_text=text_013 type=Classification\n",
      "2026-01-23 15:54:32,890 [WARNING] Eval failed task=task_013 score=0.65 issues=['Includes unnecessary `reason` field under `evidence_quotes` (should only contain `text`).', \"Missing direct evidence for the second quote (`'Please fix this soon'`).\"]\n",
      "2026-01-23 15:54:32,892 [INFO] Iteration start task=task_013 iter=1\n",
      "2026-01-23 15:54:36,283 [WARNING] Reject prompt task=task_013 iter=1 score=0.50 issues=[\"Includes irrelevant quotes ('Setup was straightforward', 'Please fix this soon') that do not contribute to the negative sentiment.\", 'Does not match exact schema for `evidence` field (quotes are wrapped in objects instead of simple arrays).', 'Extra field `sentiment_quotes` is present instead of just `evidence`.']\n",
      "2026-01-23 15:54:36,283 [INFO] Iteration start task=task_013 iter=2\n",
      "2026-01-23 15:54:39,131 [WARNING] Reject prompt task=task_013 iter=2 score=0.30 issues=[\"Includes redundant evidence ('and it’s wasting my time' is already implied by 'the sync keeps failing and it’s wasting my time')\", \"Missing punctuation correction for the second evidence item (should be 'it’s wasting my time' without ellipsis)\", 'Format inconsistency: quotes around evidence are not strictly necessary in JSON when using commas']\n",
      "2026-01-23 15:54:39,131 [INFO] End task id_task=task_013 status=no_improvement iterations_used=2\n",
      "2026-01-23 15:54:39,131 [INFO] Task 13/16 (81%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:39,132 [INFO] Task 14/16 (87%) - Processing task_id=task_014\n",
      "2026-01-23 15:54:39,132 [INFO] Start task id_task=task_014 id_prompt=prompt_014 id_text=text_014 type=Math\n",
      "2026-01-23 15:54:42,504 [WARNING] Eval failed task=task_014 score=0.60 issues=['Subtotal fields (`notebooks`, `pensets`) are incorrect and should be omitted (only `shipping` should remain).', 'Tax amount has 4 decimals instead of 2.', 'Grand total before tax is misplaced under subtotal; it should not appear there.', 'Missing required field: `tax` (should match `tax_amount`).']\n",
      "2026-01-23 15:54:42,506 [INFO] Iteration start task=task_014 iter=1\n",
      "2026-01-23 15:54:43,248 [WARNING] Sanity check failed task=task_014 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:54:43,249 [INFO] Iteration start task=task_014 iter=2\n",
      "2026-01-23 15:54:44,249 [WARNING] Sanity check failed task=task_014 iter=2 reason=too_large_increase\n",
      "2026-01-23 15:54:44,249 [INFO] End task id_task=task_014 status=sanity_check:too_large_increase iterations_used=2\n",
      "2026-01-23 15:54:44,250 [INFO] Task 14/16 (87%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:44,251 [INFO] Task 15/16 (93%) - Processing task_id=task_015\n",
      "2026-01-23 15:54:44,251 [INFO] Start task id_task=task_015 id_prompt=prompt_015 id_text=text_015 type=Extraction\n",
      "2026-01-23 15:54:47,293 [WARNING] Eval failed task=task_015 score=0.75 issues=['Missing sorting by date (events are already chronological but should be explicitly confirmed as such).', \"Incorrect phrasing for 'Internal alpha release' → 'opened internal alpha' (more natural phrasing).\", \"Inconsistent verb tense/structure: 'Bug bash' is a noun phrase, not an action; better to say 'bug bash happened' or similar.\", 'The format should use colons (`:`) after dates for clarity.']\n",
      "2026-01-23 15:54:47,294 [INFO] Iteration start task=task_015 iter=1\n",
      "2026-01-23 15:54:50,261 [WARNING] Reject prompt task=task_015 iter=1 score=0.75 issues=[\"Line for 'bug bash event scheduled' should be 'bug bash happened' (meaning mismatch)\", \"Format of 'internal alpha release opened' is slightly off: should be 'opened internal alpha' (word order differs from expected)\"]\n",
      "2026-01-23 15:54:50,261 [INFO] Iteration start task=task_015 iter=2\n",
      "2026-01-23 15:54:54,221 [WARNING] Reject prompt task=task_015 iter=2 score=0.50 issues=['Missing sorting by date (events are not chronologically ordered).', \"Incorrect format for 'internal alpha release' entry—should be 'opened internal alpha'.\", \"The phrase '{event} happened at {date}' is too generic; it does not match the expected action-oriented phrasing.\", \"Some events lack a clear action verb (e.g., 'Bug bash' should likely include an explicit action like 'held').\"]\n",
      "2026-01-23 15:54:54,222 [INFO] End task id_task=task_015 status=no_improvement iterations_used=2\n",
      "2026-01-23 15:54:54,222 [INFO] Task 15/16 (93%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:54,223 [INFO] Task 16/16 (100%) - Processing task_id=task_016\n",
      "2026-01-23 15:54:54,223 [INFO] Start task id_task=task_016 id_prompt=prompt_016 id_text=text_016 type=Product\n",
      "2026-01-23 15:54:57,401 [WARNING] Eval failed task=task_016 score=0.65 issues=[\"Line 1: Redundant repetition of 'As a **customer**' (should be merged into one statement).\", \"Line 2: Missing word 'and' between actions ('freeze/unfreeze my card instantly if lost/theft occurs').\", \"Line 3: Incorrect phrasing for push notifications ('push notifications for large transactions (>200 EUR)' → should be 'push alerts for transactions over 200 EUR').\"]\n",
      "2026-01-23 15:54:57,403 [INFO] Iteration start task=task_016 iter=1\n",
      "2026-01-23 15:54:58,198 [WARNING] Sanity check failed task=task_016 iter=1 reason=too_large_increase\n",
      "2026-01-23 15:54:58,199 [INFO] Iteration start task=task_016 iter=2\n",
      "2026-01-23 15:54:59,155 [WARNING] Sanity check failed task=task_016 iter=2 reason=too_large_increase\n",
      "2026-01-23 15:54:59,156 [INFO] End task id_task=task_016 status=sanity_check:too_large_increase iterations_used=2\n",
      "2026-01-23 15:54:59,157 [INFO] Task 16/16 (100%) - Completed: iterations=2, token_delta=+0, leakage=NO\n",
      "2026-01-23 15:54:59,157 [INFO] All tasks completed. Writing results...\n",
      "2026-01-23 15:54:59,163 [INFO] Pipeline complete. Results written to: /Users/bru1t/Documents/Development/projects/prompt-evolver/data/example.results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline complete! Results saved to: /Users/bru1t/Documents/Development/projects/prompt-evolver/data/example.results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_task</th>\n",
       "      <th>id_text</th>\n",
       "      <th>id_prompt</th>\n",
       "      <th>prompt_original</th>\n",
       "      <th>prompt_improved</th>\n",
       "      <th>tokens_original</th>\n",
       "      <th>tokens_improved</th>\n",
       "      <th>tokens_delta</th>\n",
       "      <th>iterations_used</th>\n",
       "      <th>output_original</th>\n",
       "      <th>...</th>\n",
       "      <th>output_tokens_original</th>\n",
       "      <th>output_tokens_improved</th>\n",
       "      <th>evaluation_original</th>\n",
       "      <th>evaluation_improved</th>\n",
       "      <th>model_task</th>\n",
       "      <th>model_improve</th>\n",
       "      <th>model_eval</th>\n",
       "      <th>leakage_flag</th>\n",
       "      <th>sanity_check_details</th>\n",
       "      <th>failure_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>task_001</td>\n",
       "      <td>text_001</td>\n",
       "      <td>prompt_001</td>\n",
       "      <td>Summarize the text in 3 bullet points.</td>\n",
       "      <td>Summarize **{text}** in 3 clear bullet points.</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>- **Iterative prompt refinement**: Prompt Evol...</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>87</td>\n",
       "      <td>{\"pass\": false, \"score\": 0.0, \"issues\": [\"inva...</td>\n",
       "      <td>{\"pass\": true, \"score\": 0.95, \"issues\": [\"Mino...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>task_002</td>\n",
       "      <td>text_002</td>\n",
       "      <td>prompt_002</td>\n",
       "      <td>Extract the user's name and status in JSON.</td>\n",
       "      <td>Extract `{user_name}` and `status` from `{text...</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>```json\\n{\\n  \"name\": \"Alice Nguyen\",\\n  \"stat...</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>{\"pass\": false, \"score\": 0.0, \"issues\": [\"Miss...</td>\n",
       "      <td>{\"pass\": true, \"score\": 1.0, \"issues\": [], \"su...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>task_003</td>\n",
       "      <td>text_003</td>\n",
       "      <td>prompt_003</td>\n",
       "      <td>Rewrite the text to be clearer and shorter.</td>\n",
       "      <td>Rewrite the text to be clearer and shorter.</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>The product delivers significantly faster perf...</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>{\"pass\": false, \"score\": 0.65, \"issues\": [\"The...</td>\n",
       "      <td>{\"pass\": false, \"score\": 0.65, \"issues\": [\"The...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td>no_improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>task_004</td>\n",
       "      <td>text_004</td>\n",
       "      <td>prompt_004</td>\n",
       "      <td>Compare the two products in the text and list ...</td>\n",
       "      <td>Compare the two products in the text and list ...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1. **Warranty Duration**: Product A has a **2-...</td>\n",
       "      <td>...</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>{\"pass\": false, \"score\": 0.6, \"issues\": [\"Miss...</td>\n",
       "      <td>{\"pass\": false, \"score\": 0.6, \"issues\": [\"Miss...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no</td>\n",
       "      <td>too_large_increase</td>\n",
       "      <td>no_improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>task_005</td>\n",
       "      <td>text_005</td>\n",
       "      <td>prompt_005</td>\n",
       "      <td>Evaluate the text for policy compliance and an...</td>\n",
       "      <td>Evaluate the text for policy compliance and an...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"pass\": true, \"score\": 1.0, \"issues\": [], \"su...</td>\n",
       "      <td>{\"pass\": true, \"score\": 1.0, \"issues\": [], \"su...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_task   id_text   id_prompt  \\\n",
       "0  task_001  text_001  prompt_001   \n",
       "1  task_002  text_002  prompt_002   \n",
       "2  task_003  text_003  prompt_003   \n",
       "3  task_004  text_004  prompt_004   \n",
       "4  task_005  text_005  prompt_005   \n",
       "\n",
       "                                     prompt_original  \\\n",
       "0             Summarize the text in 3 bullet points.   \n",
       "1        Extract the user's name and status in JSON.   \n",
       "2        Rewrite the text to be clearer and shorter.   \n",
       "3  Compare the two products in the text and list ...   \n",
       "4  Evaluate the text for policy compliance and an...   \n",
       "\n",
       "                                     prompt_improved  tokens_original  \\\n",
       "0     Summarize **{text}** in 3 clear bullet points.                8   \n",
       "1  Extract `{user_name}` and `status` from `{text...               11   \n",
       "2        Rewrite the text to be clearer and shorter.                9   \n",
       "3  Compare the two products in the text and list ...               12   \n",
       "4  Evaluate the text for policy compliance and an...               12   \n",
       "\n",
       "   tokens_improved  tokens_delta  iterations_used  \\\n",
       "0               14             6                1   \n",
       "1               38            27                1   \n",
       "2                9             0                2   \n",
       "3               12             0                2   \n",
       "4               12             0                1   \n",
       "\n",
       "                                     output_original  ...  \\\n",
       "0  - **Iterative prompt refinement**: Prompt Evol...  ...   \n",
       "1  ```json\\n{\\n  \"name\": \"Alice Nguyen\",\\n  \"stat...  ...   \n",
       "2  The product delivers significantly faster perf...  ...   \n",
       "3  1. **Warranty Duration**: Product A has a **2-...  ...   \n",
       "4                                               pass  ...   \n",
       "\n",
       "  output_tokens_original  output_tokens_improved  \\\n",
       "0                     91                      87   \n",
       "1                     39                      25   \n",
       "2                     61                      61   \n",
       "3                    105                     105   \n",
       "4                      1                       1   \n",
       "\n",
       "                                 evaluation_original  \\\n",
       "0  {\"pass\": false, \"score\": 0.0, \"issues\": [\"inva...   \n",
       "1  {\"pass\": false, \"score\": 0.0, \"issues\": [\"Miss...   \n",
       "2  {\"pass\": false, \"score\": 0.65, \"issues\": [\"The...   \n",
       "3  {\"pass\": false, \"score\": 0.6, \"issues\": [\"Miss...   \n",
       "4  {\"pass\": true, \"score\": 1.0, \"issues\": [], \"su...   \n",
       "\n",
       "                                 evaluation_improved model_task model_improve  \\\n",
       "0  {\"pass\": true, \"score\": 0.95, \"issues\": [\"Mino...                            \n",
       "1  {\"pass\": true, \"score\": 1.0, \"issues\": [], \"su...                            \n",
       "2  {\"pass\": false, \"score\": 0.65, \"issues\": [\"The...                            \n",
       "3  {\"pass\": false, \"score\": 0.6, \"issues\": [\"Miss...                            \n",
       "4  {\"pass\": true, \"score\": 1.0, \"issues\": [], \"su...                            \n",
       "\n",
       "  model_eval leakage_flag sanity_check_details  failure_reason  \n",
       "0                      no                                       \n",
       "1                      no                                       \n",
       "2                      no                       no_improvement  \n",
       "3                      no   too_large_increase  no_improvement  \n",
       "4                      no                                       \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompt_evolver.config import load_config\n",
    "from prompt_evolver.pipeline import run_pipeline\n",
    "\n",
    "# Select data source\n",
    "if USE_EXAMPLE_DATA:\n",
    "    prompts_path, texts_path = EXAMPLE_PROMPTS, EXAMPLE_TEXTS\n",
    "    tasks_path, results_path = EXAMPLE_TASKS, EXAMPLE_RESULTS\n",
    "else:\n",
    "    prompts_path, texts_path = CUSTOM_PROMPTS, CUSTOM_TEXTS\n",
    "    tasks_path, results_path = CUSTOM_TASKS, CUSTOM_RESULTS\n",
    "\n",
    "print(f\"Running pipeline with {prompts_path.name}...\\n\")\n",
    "\n",
    "# Load configuration and templates\n",
    "config = load_config(CONFIG_FILE)\n",
    "execution_template = EXECUTION_TEMPLATE.read_text(encoding=\"utf-8\")\n",
    "improvement_template = IMPROVEMENT_TEMPLATE.read_text(encoding=\"utf-8\")\n",
    "evaluation_template = EVALUATION_TEMPLATE.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Run pipeline\n",
    "results_df = run_pipeline(\n",
    "    prompts_path,\n",
    "    texts_path,\n",
    "    tasks_path,\n",
    "    results_path,\n",
    "    config=config,\n",
    "    execution_prompt_template=execution_template,\n",
    "    improvement_prompt_template=improvement_template,\n",
    "    evaluation_prompt_template=evaluation_template,\n",
    "    max_generations=MAX_GENERATIONS,\n",
    "    execution_model=EXECUTION_MODEL,\n",
    "    improvement_model=IMPROVEMENT_MODEL,\n",
    "    evaluation_model=EVALUATION_MODEL,\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline complete! Results saved to: {results_path}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ea10c",
   "metadata": {},
   "source": [
    "## 4. Analyze Results\n",
    "---\n",
    "\n",
    "Review the optimization results and compare before/after performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdcbcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 16:06:11,896 [INFO] Start analysis results_path=not_set\n",
      "2026-01-23 16:06:11,897 [INFO] Loading results file=/Users/bru1t/Documents/Development/projects/prompt-evolver/data/example.results.csv\n",
      "2026-01-23 16:06:11,900 [INFO] Loaded results rows=16 columns=21\n",
      "2026-01-23 16:06:11,900 [INFO] Computing summary statistics...\n",
      "2026-01-23 16:06:11,900 [INFO] Total tasks count=16\n",
      "2026-01-23 16:06:11,901 [INFO] Successful optimizations count=0 rate=0.0%\n",
      "2026-01-23 16:06:11,901 [INFO] Leakage detected count=0\n",
      "2026-01-23 16:06:11,913 [INFO] Average iterations avg=1.81\n",
      "2026-01-23 16:06:11,914 [INFO] Token delta total=+70 original_sum=143\n",
      "2026-01-23 16:06:11,914 [INFO] Token increase tokens=70 increase=49.0%\n",
      "2026-01-23 16:06:11,915 [INFO] Summary statistics complete\n",
      "2026-01-23 16:06:11,915 [INFO] Generating before/after comparison...\n",
      "2026-01-23 16:06:11,916 [INFO] Processing comparison task_id=task_001\n",
      "2026-01-23 16:06:11,916 [INFO] Original prompt chars=38\n",
      "2026-01-23 16:06:11,916 [INFO] Improved prompt chars=46\n",
      "2026-01-23 16:06:11,917 [INFO] Task metrics task_id=task_001 tokens_original=8 tokens_improved=14 delta=+6 iterations=1 leakage=no\n",
      "2026-01-23 16:06:11,917 [INFO] Parsing evaluation scores task_id=task_001\n",
      "2026-01-23 16:06:11,917 [INFO] Evaluation scores task_id=task_001 original=0.00 improved=0.95 delta=+0.95\n",
      "2026-01-23 16:06:11,917 [INFO] Comparison complete task_id=task_001\n",
      "2026-01-23 16:06:11,918 [INFO] Preparing detailed results table...\n",
      "2026-01-23 16:06:11,918 [INFO] Displaying results columns=6 rows=16/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "Total tasks processed: 16\n",
      "Successful optimizations: 0 (0.0%)\n",
      "Tasks flagged for leakage: 0\n",
      "Average iterations per task: 1.8\n",
      "Total token change: +70 tokens\n",
      "Token increase: 70 tokens (+49.0%)\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE: BEFORE/AFTER COMPARISON (Task 1)\n",
      "======================================================================\n",
      "\n",
      "ORIGINAL PROMPT:\n",
      "----------------------------------------------------------------------\n",
      "Summarize the text in 3 bullet points.\n",
      "\n",
      "IMPROVED PROMPT:\n",
      "----------------------------------------------------------------------\n",
      "Summarize **{text}** in 3 clear bullet points.\n",
      "\n",
      "METRICS:\n",
      "----------------------------------------------------------------------\n",
      "  Tokens: 8 → 14 (Δ+6)\n",
      "  Iterations: 1\n",
      "  Leakage detected: no\n",
      "  Original score: 0.00\n",
      "  Improved score: 0.95\n",
      "\n",
      "======================================================================\n",
      "DETAILED RESULTS\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_task</th>\n",
       "      <th>tokens_original</th>\n",
       "      <th>tokens_improved</th>\n",
       "      <th>tokens_delta</th>\n",
       "      <th>iterations_used</th>\n",
       "      <th>leakage_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>task_001</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>task_002</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>task_003</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>task_004</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>task_005</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>task_006</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>task_007</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>task_008</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>task_009</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>task_010</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>task_011</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>task_012</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>task_013</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>task_014</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>task_015</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>task_016</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_task  tokens_original  tokens_improved  tokens_delta  iterations_used  \\\n",
       "0   task_001                8               14             6                1   \n",
       "1   task_002               11               38            27                1   \n",
       "2   task_003                9                9             0                2   \n",
       "3   task_004               12               12             0                2   \n",
       "4   task_005               12               12             0                1   \n",
       "5   task_006               13               50            37                2   \n",
       "6   task_007               11               11             0                2   \n",
       "7   task_008                9                9             0                2   \n",
       "8   task_009                7                7             0                2   \n",
       "9   task_010                7                7             0                2   \n",
       "10  task_011                6                6             0                2   \n",
       "11  task_012                9                9             0                2   \n",
       "12  task_013                7                7             0                2   \n",
       "13  task_014                5                5             0                2   \n",
       "14  task_015               10               10             0                2   \n",
       "15  task_016                7                7             0                2   \n",
       "\n",
       "   leakage_flag  \n",
       "0            no  \n",
       "1            no  \n",
       "2            no  \n",
       "3            no  \n",
       "4            no  \n",
       "5            no  \n",
       "6            no  \n",
       "7            no  \n",
       "8            no  \n",
       "9            no  \n",
       "10           no  \n",
       "11           no  \n",
       "12           no  \n",
       "13           no  \n",
       "14           no  \n",
       "15           no  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 16:06:11,923 [INFO] Analysis complete total_tasks=16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# === Configure logging (same format as pipeline) ===\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"prompt_evolver\")\n",
    "\n",
    "# === Load Results ===\n",
    "logger.info(\"Start analysis results_path=%s\", results_path if 'results_path' in dir() else 'not_set')\n",
    "\n",
    "results_path = EXAMPLE_RESULTS if USE_EXAMPLE_DATA else CUSTOM_RESULTS\n",
    "logger.info(\"Loading results file=%s\", results_path)\n",
    "\n",
    "try:\n",
    "    results = pd.read_csv(results_path)\n",
    "    logger.info(\"Loaded results rows=%d columns=%d\", len(results), len(results.columns))\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Results file not found path=%s\", results_path)\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to load results error=%s\", e)\n",
    "    raise\n",
    "\n",
    "# === Summary Statistics ===\n",
    "logger.info(\"Computing summary statistics...\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_tasks = len(results)\n",
    "logger.info(\"Total tasks count=%d\", total_tasks)\n",
    "\n",
    "successful = (results['failure_reason'] == '').sum()\n",
    "logger.info(\"Successful optimizations count=%d rate=%.1f%%\", successful, successful/total_tasks*100)\n",
    "\n",
    "with_leakage = (results['leakage_flag'] == 'yes').sum()\n",
    "logger.info(\"Leakage detected count=%d\", with_leakage)\n",
    "\n",
    "avg_iterations = results['iterations_used'].mean()\n",
    "logger.info(\"Average iterations avg=%.2f\", avg_iterations)\n",
    "\n",
    "total_token_delta = results['tokens_delta'].sum()\n",
    "tokens_original_sum = results['tokens_original'].sum()\n",
    "logger.info(\"Token delta total=%+d original_sum=%d\", total_token_delta, tokens_original_sum)\n",
    "\n",
    "print(f\"Total tasks processed: {total_tasks}\")\n",
    "print(f\"Successful optimizations: {successful} ({successful/total_tasks*100:.1f}%)\")\n",
    "print(f\"Tasks flagged for leakage: {with_leakage}\")\n",
    "print(f\"Average iterations per task: {avg_iterations:.1f}\")\n",
    "print(f\"Total token change: {total_token_delta:+d} tokens\")\n",
    "\n",
    "if total_token_delta < 0:\n",
    "    savings_pct = abs(total_token_delta) / tokens_original_sum * 100\n",
    "    print(f\"Token savings: {abs(total_token_delta)} tokens ({savings_pct:.1f}% reduction)\")\n",
    "    logger.info(\"Token savings tokens=%d reduction=%.1f%%\", abs(total_token_delta), savings_pct)\n",
    "elif total_token_delta > 0:\n",
    "    increase_pct = total_token_delta / tokens_original_sum * 100\n",
    "    print(f\"Token increase: {total_token_delta} tokens (+{increase_pct:.1f}%)\")\n",
    "    logger.info(\"Token increase tokens=%d increase=%.1f%%\", total_token_delta, increase_pct)\n",
    "else:\n",
    "    logger.info(\"Token delta unchanged total=0\")\n",
    "\n",
    "logger.info(\"Summary statistics complete\")\n",
    "\n",
    "# === Before/After Comparison (First Task) ===\n",
    "logger.info(\"Generating before/after comparison...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE: BEFORE/AFTER COMPARISON (Task 1)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(results) > 0:\n",
    "    task = results.iloc[0]\n",
    "    task_id = task.get('id_task', 'unknown')\n",
    "    logger.info(\"Processing comparison task_id=%s\", task_id)\n",
    "    \n",
    "    print(\"\\nORIGINAL PROMPT:\")\n",
    "    print(\"-\" * 70)\n",
    "    prompt_original_len = len(task['prompt_original'])\n",
    "    prompt_preview = task['prompt_original'][:150] + \"...\" if prompt_original_len > 150 else task['prompt_original']\n",
    "    print(prompt_preview)\n",
    "    logger.info(\"Original prompt chars=%d\", prompt_original_len)\n",
    "    \n",
    "    print(\"\\nIMPROVED PROMPT:\")\n",
    "    print(\"-\" * 70)\n",
    "    prompt_improved_len = len(task['prompt_improved'])\n",
    "    improved_preview = task['prompt_improved'][:150] + \"...\" if prompt_improved_len > 150 else task['prompt_improved']\n",
    "    print(improved_preview)\n",
    "    logger.info(\"Improved prompt chars=%d\", prompt_improved_len)\n",
    "    \n",
    "    print(\"\\nMETRICS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Tokens: {task['tokens_original']} → {task['tokens_improved']} (Δ{task['tokens_delta']:+d})\")\n",
    "    print(f\"  Iterations: {task['iterations_used']}\")\n",
    "    print(f\"  Leakage detected: {task['leakage_flag']}\")\n",
    "    logger.info(\n",
    "        \"Task metrics task_id=%s tokens_original=%d tokens_improved=%d delta=%+d iterations=%d leakage=%s\",\n",
    "        task_id, task['tokens_original'], task['tokens_improved'], \n",
    "        task['tokens_delta'], task['iterations_used'], task['leakage_flag']\n",
    "    )\n",
    "    \n",
    "    # Parse evaluation scores\n",
    "    logger.info(\"Parsing evaluation scores task_id=%s\", task_id)\n",
    "    try:\n",
    "        eval_orig = json.loads(task['evaluation_original'])\n",
    "        eval_improved = json.loads(task['evaluation_improved'])\n",
    "        print(f\"  Original score: {eval_orig['score']:.2f}\")\n",
    "        print(f\"  Improved score: {eval_improved['score']:.2f}\")\n",
    "        score_delta = eval_improved['score'] - eval_orig['score']\n",
    "        logger.info(\n",
    "            \"Evaluation scores task_id=%s original=%.2f improved=%.2f delta=%+.2f\",\n",
    "            task_id, eval_orig['score'], eval_improved['score'], score_delta\n",
    "        )\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.warning(\"JSON parse failed task_id=%s error=%s\", task_id, e)\n",
    "    except KeyError as e:\n",
    "        logger.warning(\"Missing evaluation key task_id=%s key=%s\", task_id, e)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Evaluation parse error task_id=%s error=%s\", task_id, e)\n",
    "    \n",
    "    logger.info(\"Comparison complete task_id=%s\", task_id)\n",
    "else:\n",
    "    logger.warning(\"No results available for comparison rows=0\")\n",
    "\n",
    "# === Detailed Results Table ===\n",
    "logger.info(\"Preparing detailed results table...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "columns_to_display = ['id_task', 'tokens_original', 'tokens_improved', \n",
    "                      'tokens_delta', 'iterations_used', 'leakage_flag']\n",
    "rows_to_show = min(20, len(results))\n",
    "logger.info(\"Displaying results columns=%d rows=%d/%d\", len(columns_to_display), rows_to_show, len(results))\n",
    "\n",
    "display(results[columns_to_display].head(20))\n",
    "\n",
    "logger.info(\"Analysis complete total_tasks=%d\", total_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cbc67c",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "---\n",
    "\n",
    "### Use Your Own Data\n",
    "\n",
    "To optimize your custom prompts:\n",
    "1. Create CSV files: `data/prompts.csv`, `data/texts.csv`, `data/tasks.csv`\n",
    "2. Set `USE_EXAMPLE_DATA = False` in Section 2\n",
    "3. Re-run the pipeline (Section 3)\n",
    "\n",
    "See [docs/data-model.md](../docs/data-model.md) for CSV format details.\n",
    "\n",
    "### Run Tests\n",
    "\n",
    "Validate the codebase with unit tests:\n",
    "```bash\n",
    "pytest\n",
    "```\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Overview](../docs/overview.md) - Core concepts and workflow\n",
    "- [Pipeline](../docs/pipeline.md) - Optimization loop details  \n",
    "- [Config](../docs/config.md) - Configuration reference\n",
    "- [Prompts](../docs/prompts.md) - Prompt template guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
